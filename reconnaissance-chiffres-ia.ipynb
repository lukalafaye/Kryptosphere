{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconnaissance de chiffres manuscrits avec scikit-learn\n",
    "@ Colin Bernet IP2I Lyon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Échantillon de données de chiffres manuscrits\n",
    "\n",
    "Une version basse résolution de cet échantillon est fourni avec scikit-learn.\n",
    "\n",
    "On commence par charger l'échantillon : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis on imprime la première image : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  3. 14. 15.  6.  0.  0.  0.]\n",
      " [ 0.  7. 15. 14. 15.  0.  0.  0.]\n",
      " [ 0.  2.  7.  2. 14.  3.  0.  0.]\n",
      " [ 0.  0.  0.  1. 14.  4.  0.  0.]\n",
      " [ 0.  0.  0.  7. 15.  2.  0.  0.]\n",
      " [ 0.  0.  5. 15. 14.  4.  1.  0.]\n",
      " [ 0.  4. 15. 16. 16. 16.  6.  0.]\n",
      " [ 0.  4. 15. 13. 12. 11.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(digits.images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme toutes les images de l'échantillon, celle-ci est une image de 8x8 pixels, noir et blanc (un seul niveau de couleur par pixel). On peut l'afficher de la manière suivante, en indiquant également l'étiquette correspondante (le chiffre auquel correspond l'image) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAEICAYAAAByNDmmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAABPVJREFUeJzt3bFqXdkVgOG1B4d0mdtNaVWBdH4Dq0gz09iPoG5av0Cw9AbuUk05KQIDcpVWfoKIwJQDDnkCkyKEBE6qQDoHonuO8uv7Wh3OWoV+9kVXsNe2bQM0fXH0AsD5CBzCBA5hAocwgUOYwCFM4BAm8CdqrfXztdZ3a60/r7X+utb641rr66P34mEJ/Ol6NjN/mZmXM/PlzPxmZn6/1ro4cCce2PKfbPzbWutPM3OzbdsPR+/Cw3CCMzMza62vZuaXM/Pj0bvwcJzgzFrrZzPzh5n5adu2b4/eh4cj8CdurfXFzPxuZn4xM6+2bfvHwSvxgJ4dvQDHWWutmfluZr6amW/E3SPwp+23M/Ormfn1tm1/O3oZHp6P6E/UWuv5zHycmb/PzD//40ffbtv2/SFL8eAEDmG+JoMwgUOYwCFM4BB2rq/Jkn+5u7u723Xe7e3tbrPu7+93m/Xhw4fdZr19+3a3WTMz19fXe45bn3vACQ5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAoewc90Pnry66HQ6Hb3C2bx58+boFc7i6upq13kXFxd7jnN1ETxlAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAoewZ0cv8L+6u7vbbdanT592mzUz8+rVq91mXV5e7jbrxYsXu80qXzf133CCQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgEOZuskfs/fv3yVnPnz/fbdbHjx93m/UYOcEhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQtrZtO8d7z/LSo71+/XrXeRcXF7vNOp1Ou826ubnZbdaZfr8fi/W5B5zgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCHt29AL/T25vb49e4WzevXt39AqcgRMcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYa4uesSur693m3Vzc7PbrJcvX+4266lzgkOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BC2tm07x3vP8tKj3d/f7zrv6upqt1mXl5e7zdrzzrXT6bTbrAOszz3gBIcwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUPYua4uAh4BJziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQ9i/8jn6Cebb85AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(digits.images[0],cmap='binary')\n",
    "plt.title(digits.target[0])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons entraîner un réseau de neurones simple à reconnaître les chiffres dans ces images. Ce réseau va prendre en entrée des tableaux 1D de 8x8=64 valeurs. Nous devons donc convertir nos images 2D en tableaux 1D :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "x = digits.images.reshape((len(digits.images), -1))\n",
    "# x contient toutes les images en version 1D.\n",
    "# nous imprimons ici la première, que nous avons déjà vue : \n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le réseau va agir comme une fonction permettant de passer d'un tableau de 64 valeurs en entrée à une valeur en sortie, son estimation du chiffre. Voici les valeurs de sortie : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(digits.images)) # Nombre d'images dans le set d'échantillons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition et entraînement du réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous décidons de créer un réseau de neurones relativement simple, avec une seule couche de 15 neurones : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(15,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons entraîner ce réseau sur les 1000 premières images de notre échantillon, et réserver les images suivantes pour tester les performances du réseau : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x[:1000]\n",
    "y_train = y[:1000]\n",
    "x_test = x[1000:]\n",
    "y_test = y[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(15,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà ! nous pouvons maintenant regarder ce que donne le réseau pour les images suivantes, qui n'ont pas été vues par le réseau lors de l'entraînement : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 0, 5, 3, 6, 9, 6, 1, 7])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.predict(x_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 0, 5, 3, 6, 9, 6, 1, 7])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les 10 premières images de test, les estimations sont excellentes ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant évaluer le réseau pour toutes les images de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11794228356336262"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = mlp.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis rechercher les images pour lesquelles le réseau s'est trompé : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = (y_pred != y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le calcul du taux d'erreur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11794228356336262"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum(error) / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "environ 11%, ce qui veut dire que 89% des prédictions sont correctes: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons enfin sélectionner les mauvaises prédictions pour les afficher :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_error = x_test[error].reshape((-1, 8,8))\n",
    "y_error = y_test[error]\n",
    "y_pred_error = y_pred[error]\n",
    "i = 1\n",
    "plt.imshow(x_error[i],cmap='binary')\n",
    "plt.title(f'cible: {y_error[i]}, prediction: {y_pred_error[i]}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir, il est difficile de classifier ces images, même pour un humain.\n",
    "\n",
    "Pour de meilleures performances, il faudrait utiliser des images de plus haute résolution et un réseau de neurones plus complexe, comme un réseau convolutionnel.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
